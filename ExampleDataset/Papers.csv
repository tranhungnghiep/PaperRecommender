11|||Indri at TREC 2004: Terabyte Track|||This paper provides an overview of experiments carried out at the TREC 2004 Terabyte Track using the Indri search engine. Indri is an efficient, effective distributed search engine. Like INQUERY, it is based on the inference network framework and supports structured queries, but unlike INQUERY, it uses lan- guage modeling probabilities within the network which allows for added flexibility. We describe our approaches to the Ter- abyte Track, all of which involved automatically constructing structured queries from the title portions of the TREC topics. Our methods use term proximity information and HTML doc- ument structure. In addition, a number of optimization proce- dures for efficient query processing are explained|||2004
111|||Language Models for Information Retrieval|||One of the major challenges in the field of information retrieval (IR) is to specify a formal framework that both describes the important processes involved in finding relevant information, and successfully predicts which techniques will provide good effectiveness in terms of accuracy. A recent approach that has shown considerable promise uses generative models of text (language models) to describe the IR processes. We briefly review the major variations of the language model approach and how they have been used to develop a range of retrieval-related language technologies, including cross-lingual IR and distributed search. We also discuss how this approach could be used with structured data extracted from text.|||2003
112|||Combining document representations for known-item search|||This paper investigates the pre-conditions for successful combination of document representations formed from structural markup for the task of known-item search. As this task is very similar to work in meta-search and data fusion, we adapt several hypotheses from those research areas and investigate them in this context. To investigate these hypotheses, we present a mixture-based language model and also examine many of the current meta-search algorithms. We find that compatible output from systems is important for successful combination of document representations. We also demonstrate that combining low performing document representations can improve performance, but not consistently. We find that the techniques best suited for this task are robust to the inclusion of poorly performing document representations. We also explore the role of variance of results across systems and its impact on the performance of fusion, with the surprising result that the correct documents have higher variance across document representations than highly ranking incorrect documents.|||2003
113|||Integration of multiple evidences based on a query type for web search|||The massive and heterogeneous Web exacerbates IR problems and short user queries make them worse. The contents of web pages are not enough to find answer pages. PageRank compensates for the insufficiencies of content information. The content information and PageRank are combined to get better results. However, static combination of multiple evidences may lower the retrieval performance.We have to use different strategies to meet the need of a user. We can classify user queries as three categories according to users' intent, the topic relevance task, the homepage finding task, and the service finding task. In this paper, we present a user query classification method. The difference of distribution, mutual information, the usage rate as anchor texts and the POS information are used for the classification. After we classified a user query, we apply different algorithms and information for the better results. For the topic relevance task, we emphasize the content information, on the other hand, for the homepage finding task, we emphasize the Link information and the URL information. We could get the best performance when our proposed classification method with the OKAPI scoring algorithm was used.|||2004
114|||Formal multiple-bernoulli models for language modeling|||Abstract: In language modeling, it is nearly always assumed that documents are generated by sampling from a multinomial distribution. Many formal methods of estimation exist for the multinomial case. In this paper, we reexamine language models based on a multiple-Bernoulli distribution. This assumption has been explored in the past, but has never been formalized. Here, we present how smoothed language models can be estimated using this assumption from a formal, Bayesian standpoint|||2004
115|||A language modeling approach to information retrieval|||Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem.We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either.Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have implemented our model and tested it empirically. Our approach significantly outperforms standard tf.idf weighting on two different collections and query sets.|||1998
116|||Efficient single-pass index construction for text databases|||Ecien t construction of inverted indexes is essential to provision of search over large collections of text data. In this paper, we review the principal approaches to inversion, analyse their theoretical cost, and present experimental results. We identify the drawbacks of existing inversion approaches and propose a single-pass inversion method that, in contrast to previous approaches, does not require the complete vocabulary of the indexed collection in main memory, can operate within limited resources, and does not sacrice speed with high temporary storage requirements. We show that the performance of the single-pass approach can be improved by constructing inverted les in segments, reducing the cost of disk accesses during inversion of large volumes of data|||2003
117|||Overview of the TREC9 Web Track|||The TREC-8 Web Track defined ad hoc retrieval tasks over the 100 gigabyte VLC2 collection(Large Web Task) and a selected 2 gigabyte subset known as WT2g (Small Web Task). Here, theguidelines and resources for both tasks are described and results presented and analysed.Performance on the Small Web was strongly correlated with performance on the regular TRECAd Hoc task. Little benefit was derived from the use of link-based methods, for standard TRECmeasures on the WT2g collection. The...|||2000
118|||Evaluation of an inference network-based retrieval model|||The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models.|||1991
119|||Query Evaluation: Strategies and Optimizations|||This paper discusses the two major query evaluation strategies used in large text retrieval systems and analyzes the performance of these strategies. We then discuss several optimization techniques that can be used to reduce evaluation costs and present simulation results to compare the performance of these optimization techniques when evaluating natural language queries with a collection of full text legal materials.|||1995
1110|||Overview of the TREC2001 Web Track|||The TREC-2002 VVcb Track moved away from non-Web relevance ranking and towards Vcbspecifictasks on a 1.25 million page crawl &quot;.GOV&quot;. The topic distillation task involved finding pageswhich were relevant, but also had characteristics which would make them desirable inclusions in adistilled list of key pages. The nmned page task is a variant of last year's homepage finding task|||2001
1111|||A study of smoothing methods for language models applied to information retrieval|||Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to explain the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.|||2004
1112|||Combining the language model and inference network approaches to retrieval|||The inference network retrieval model, as implemented in the InQuery search engine, allows for richly structured queries. However, it incorporates a form of ad hoc tf.idf estimates for word probabilities. Language modeling offers more formal estimation techniques. In this paper we combine the language modeling and inference network approaches into a single framework. The resulting model allows structured queries to be evaluated using language modeling estimates. We explore the issues involved, such as combining beliefs and smoothing of proximity nodes. Experimental results are presented comparing the query likelihood model, the InQuery system, and our new model. The results reaffirm that high quality structured queries outperform unstructured queries and show that our system consistently achieves higher average precision than InQuery|||2004
1113|||Shortest Substring Ranking (MultiText Experiments for TREC4)|||To address the TREC-4 topics, we used a precise query language that yields and combinesarbitrary intervals of text rather than pre-defined units like words and documents. Each solutionwas scored in inverse proportion to the length of the shortest interval containing it. Eachdocument was scored by the sum of the scores of solutions within it. Whenever the abovestrategy yielded less than 1000 documents, documents satisfying successively weaker querieswere added with lower rank. Our results...|||1995
1114|||The Importance of Prior Probabilities for Entry Page Search|||An important class of searches on the world-wide-web has the goal to find an entry page (homepage) of an organisation. Entry page search is quite different from Ad Hoc search. Indeed a plain Ad Hoc system performs disappointingly. We explored three non-content features of web pages: page length, number of incoming links and URL form. Especially the URL form proved to be a good predictor. Using URL form priors we found over 70% of all entry pages at rank 1, and up to 89% in the top 10. Non-content features can easily be embedded in a language model framework as a prior probability|||2002
1115|||Relevance based language models|||We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data|||2001
12|||Optimization strategies for complex queries|||Previous research into the efficiency of text retrieval systems has dealt primarily with methods that consider inverted lists in sequence; these methods are known as term-at-a-time methods. However, the literature for optimizing document-at-a-time systems remains sparse.We present an improvement to the max_score optimization, which is the most efficient known document-at-a-time scoring method. Like max_score, our technique, called term bounded max_score, is guaranteed to return exactly the same scores and documents as an unoptimized evaluation, which is particularly useful for query model research. We simulated our technique to explore the problem space, then implemented it in Indri, our large scale language modeling search engine. Tests with the GOV2 corpus on title queries show our method to be 23% faster than max_score alone, and 61% faster than our document-at-a-time baseline. Our optimized query times are competitive with conventional term-at-a-time systems on this year's TREC Terabyte task.|||2005
121|||Managing Gigabytes: Compressing and Indexing Documents and Images||| |||1994
122|||Self-indexing inverted files for fast text retrieval|||Query-processing costs on large text databases are dominated by the need to retrieve and scan the inverted list of each query term. Retrieval time for inverted lists can be greatly reduced by the use of compression, but this adds to the CPU time required. Here we show that the CPU component of query response time for conjunctive Boolean queries and for informal ranked queries can be similarly reduced, at little cost in terms of storage, by the inclusion of an internal index in each compressed inverted list. This method has been applied in a retrieval system for a collection of nearly two million short documents. Our experimental results show that the self-indexing strategy adds less than 20% to the size of the compressed inverted file, which itself occupies less than 10% of the indexed text, yet can reduce processing time for Boolean queries of 5-10 terms to under one fifth of the previous cost. Similarly, ranked queries of 40-50 terms can be evaluated in as little as 25% of the previous time, with little or no loss of retrieval effectiveness.|||1996
123|||Fast evaluation of structured queries for information retrieval|||Information retrieval systems are being challenged to managelarger and larger document collections. In an effort to providebetter retrieval performance on large collections, moresophisticated retrieval techniques have been developed that supportrich, structured queries. Structured queries are not amenable topreviously proposed optimization techniques. Optimizing execution,however, is even more important in the context of large documentcollections. We present a new structured query optimizationtechnique which we have implemented in an inference network-basedinformation retrieval system. Experimental results show that queryevaluation time can be reduced by more than half with little impacton retrieval effectiveness|||1995
124|||Optimization of inverted vector searches|||A simple algorithm is presented for increasing the efficiency of information retrieval searches which are implemented using inverted files. This optimization algorithm employs knowledge about the methods used for weighting document and query terms in order to examine as few inverted lists as possible. An extension to the basic algorithm allows greatly increased performance optimization at a modest cost in retrieval effectiveness. Experimental runs are made examining several different term weighting models and showing the optimization possible with each.|||1985
125|||Efficient query evaluation using a two-level retrieval process|||We present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. The efficiency of the evaluation process can be improved significantly using dynamic pruning techniques with very little cost in effectiveness. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation. Experimentally, using the TREC Web Track data, we have determined that our algorithm significantly reduces the total number of full evaluations by more than 90%, almost without any loss in precision or recall. At the heart of our approach there is an efficient implementation of a new Boolean construct called WAND or Weak AND that might be of independent interest.|||2003
126|||Self-Indexing Inverted Files for Fast Text Retrieval|||Abstract Query processing costs on large text databases are dominated,by the need to retrieve and scan the inverted list of each query term. Here we show that query response time for conjunctive Boolean queries and for informal ranked queries can be dramatically reduced, at little cost in terms of storage, by the inclusion of an internal index in each inverted list. This method,has been applied in a retrieval system for a collection of nearly two million short documents. Our experimental results show that the self- indexing strategy adds less than 20% to the size of the inverted le, but, for Boolean queries of 5{10 terms, can reduce processing time to under one fth of the previous cost. Similarly, ranked queries of 40{50 terms can be evaluated in as little as 25% of the previous time, with little or no loss of retrieval eectiveness. CR Categories: E.4 [Coding and Information Theory]: data compaction and com- pression; H.3.1 [Information Storage and Retrieval] Content Analysis and Indexing| indexing methods; H.3.2 [Information Storage and Retrieval] Information Storage| le,organisation; H.3.3 [Information Storage and Retrieval] Information Search and retrieval|search process; Keywords: full-text retrieval, information retrieval, index compression, inverted le, query processing.|||\N
13|||UMass Robust 2005 Notebook: Using Mixtures of Relevance Models for Query Expansion|||UMass Robust 2005 Notebook: Using Mixtures of Relevance Models for Query Expansion|||2005
131|||A Markov random field model for term dependencies|||This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections.|||2005
21|||Challenges in information retrieval and language modeling: report of a workshop held at the center for intelligent information retrieval||| |||2003
22|||CIIR Experiments for TREC Legal 2007|||Four baseline experiments using standard In- dri retrieval facilities and simple query formula- tion techniques and two experiments using more advanced formulations (dependence models and pseudo-relevance feedback) are described. All of the experiments perform substantially better than the median performance of automatic runs but exhibit lower estimated precision and recall at B than the reference Boolean run.|||2007
221|||An evaluation of retrieval effectiveness for a full-text document-retrieval system|||An evaluation of a large, operational full-text document-retrieval system (containing roughly 350,000 pages of text) shows the system to be retrieving less than 20 percent of the documents relevant to a particular search. The findings are discussed in terms of the theory and practice of full-text document retrieval|||1985
222|||Latent concept expansion using markov random fields|||Query expansion, in the form of pseudo-relevance feedback or relevance feedback, is a common technique used to im- prove retrieval effectiveness. Most previous approaches have ignored important issues, such as the role of features and the importance of modeling term dependencies. In this paper, we propose a robust query expansion technique based on the Markov random field model for information retrieval. The technique, called latent concept expansion, provides a mechanism for modeling term dependencies during expan- sion. Furthermore, the use of arbitrary features within the model provides a powerful framework for going beyond sim- ple term occurrence features that are implicitly used by most other expansion techniques. We evaluate our tech- nique against relevance models, a state-of-the-art language modeling query expansion technique. Our model demon- strates consistent and significant improvements in retrieval effectiveness across several TREC data sets. We also de- scribe how our technique can be used to generate mean- ingful multi-term concepts for tasks such as query sugges- tion/reformulation.|||2007
223|||Indri: A language-model based search engine for complex queries1|||Search and detection technology has been a focus of DARPA and ARDA research programs since the TIPSTER program began in the early 1990s (Harman 1992). A num-ber of innovations have been developed in this research, resulting in very significant improvements in the effective-ness of search tools. The Inquery search engine (Callan et al. 1995), developed at the University of Massachusetts for the TIPSTER project, provided a query language capable of representing complex queries in a probabilistic framework and was used in a number of government and commercial applications.|||2005
31|||Using Wearable Computers to Construct Semantic Representations of Physical Spaces|||The representation of physical space has traditionally fo- cused on keyphrases such as "Computer Science Building" or "Physics Department" that help us in describing and navigating physical spaces. However, such keyphrases do not capture many properties of physical space. As with the assignment of a keyword to describe a piece of text, these constructs sacrifice meaningful information for abstraction. We propose a system of spatial representation based on richer, emergent language models that encode information lost in keyphrase approaches. We use a mix of wearable and ubiquitous computing environments for the construction of these models. Wearable computers infer language models of their hosts. These language models then act as semantic paint over spaces in a ubiquitous computing environment. Spaces collect this information and construct representa- tions based on interactions with augmented humans. A pro- totype navigation system based on this theory is presented and compared to traditional representations.|||2002
311|||Information access for context-aware appliances |||The emergence of networked context-aware mobile comput- ing appliances potentially offers opportunities for remote ac- cess to huge online information resources. Information access in context-aware information appliances can utilize existing tech- niques developed for effective information retrieval and infor- mation filtering; however, practical physical and operational fea- tures of these devices and the availability of context information itself suggest that the document selection process should make use of this contextual data.|||2000
312|||Empirical Analysis of Predictive Algorithms for Collaborative Filtering|||Collaborative filtering or recommender systemsuse a database about user preferences topredict additional topics or products a newuser might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients,vector-based similarity calculations,and statistical Bayesian methods. We comparethe predictive accuracy of the various methods in a set of representative problemdomains. We use two basic classes of evaluation...|||1998
313|||AntNet: Distributed Stigmergetic Control for Communications Networks |||This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of ...|||1998
314|||The PageRank Citation Ranking: Bringing Order to the Web|||The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.|||1998
315||| Situated Grounded Word Semantics|||The paper reports on experiments in which au- tonomous visually grounded agents bootstrap an ontology and a shared lexicon without prior design nor other forms of human intervention. The agents do so while playing ap articular lan- guage game called the guessing game. We show that synonymy and polysemy arise as emergent properties in the language but also that there are tendencies to dampen it so as to make the language more coherent and thus more optimal from the viewpoints of communicative success, cognitive complexity, and learnability.|||1999
316|||Triggering Information by Context|||With the increased availability of personal computers with attached sensors to capture their environment, there is a big opportunity forcontext-aware applications; these automatically provide information and/or take actions according to the user's present context, as detected by sensors. When well designed, these applications provide an opportunity to tailor the provision of information closely to the user's current needs. A sub-set of context-aware applications arediscrete applications, where discrete pieces of information are attached to individual contexts, to be triggered when the user enters those contexts. The advantage of discrete applications is that authoring them can be solely a creative process rather than a programming process: it can be a task akin to creating simple web pages. This paper looks at a general system that can be used in any discrete context-aware application. It propounds a general triggering rule, and investigates how this rule applies in practical applications.|||1998
317|||Content-Boosted Collaborative Filtering|||Most recommender systems use Collaborative Filtering or Content-based methods to predict new items of interest for a user. While both methods have their own advantages, individually they fail to provide good recommendations in many situations. Incorporating components from both methods, a hybrid recommender system can overcome these shortcomings. In this paper, we present an elegant and effective framework for combining content and collaboration. Our approach uses a content-based predictor to...|||2001
318|||Information Retrieval|||Information retrieval is a wide, often loosely-defined term but in these pages I shall be concerned only with automatic information retrieval systems. Automatic as opposed to manual and information as opposed to data or fact. Unfortunately the word information can be very misleading. In the context of information retrieval (IR), information, in the technical meaning given in Shannon's theory of communication, is not readily measured (Shannon and Weaver1). In fact, in many cases one can...|||1979
318|||Authoritative sources in a hyperlinked environment |||The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of context on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authorative” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristrics for link-based analysis.|||1999
319||||Recognizing User Context via Wearable Sensors||||We describe experiments in recognizing a person's situation from only a wearable camera and microphone. The types of situations considered in these experiments are coarse locations (such as at work, in a subway or in a grocery store) and coarse events (such as in a conversation or walking down a busy street) that would require only global, non-attentional features to distinguish them|||2000
3110|||What Shall We Teach Our Pants?|||If a wearable device can register what the wearer is currently doing, it can anticipate and adjust its behavior to avoid redundant interaction with the user. However, the relevance and properties of the activities that should be recognized depend on both the application and the user. This requires an adaptive recognition of the activities where the user, instead of the designer, can teach the device what he/she is doing. As a case study we connected a pair of pants with accelerometers to a laptop to interpret the raw sensor data. Using a combination of machine learning techniques such as Kohonen maps and probabilistic models, we build a system that is able to learn activities while requiring minimal user attention. This approach to context awareness is more universal since it requires no a priori knowledge about the contexts or the user.|||2000
3111|||Foundations of Statistical Natural Language Processing|||Abstract: this paperas &quot;the first clear demonstration of a probabilistic parser outperforming a trigram model&quot; (pg. 457), itdoes not discuss what features of the algorithm lead to its superior results|||1999
3112|||Just-In-Time Information Retrieval||| This thesis defines Just-In-Time Information Retrieval agents (JITIRs): a class of softwareagents that proactively present potentially valuable information based on a person's localcontext in an easily accessible yet non-intrusive manner. The research described experimentallydemonstrates that such systems encourage the viewing and use of information that would not otherwisebe viewed, by reducing the cognitive effort required to find, evaluate and access information.Experiments and...|||2000
3113|||DataSpace—querying and monitoring deeply networked collections in physical space|||The Da&pace& a three dimensional physical space 100 kilometers above and 10 kilometers below the surface of earth that is accessible to the network. It is addressed geographically as opposed to the cur- rent "logical" addressing scheme of the Internet. With the enormous 128 bit addressing space of IP version 6, one can individually address every cubic centimeter of physical space on earth with approximately 80 bits of area code. This would include every street, building, room, basement or even drawer of a desk. The Dataspace would thus serve as the host for the entire part of the physical world that is connected to the network. The billions of objects populating the Dataspace, each aware of its own geographic location2 will form "dataflocks", mobile object classes which can be selectively queried, monitored and controlled. To support the datas- pace, we propose a version of the multicast pro- tocol called "spacecast". Here, the network plays the role of a Database machine, handling queries through spacecast which "illuminate' ' selected data cubes and gather multiple responses from the ob- jects that respond to the query. The user will in- teract with the Dataspace through spacecast in the same way that a person interacts with his physical environment through the information in reflected light waves.|||1999
3114|||Informatics, Architecture and Language|||Two complementary schools of thought exist with regard to the basic underlying assumptions and philosophies that guide our research in information navigation and access. As with all of HCl, and indeed most of informatics, we can place theories and design practices based in objectivity and mathematics at one end of a spectrum, and those emphasising subjectivity and language at the other. The first school of thought sees itself as part of traditional computer science, rooted in models that encompass the individual variations of users and that are often derived from experimentation and observation in controlled conditions. Mainstream information retrieval, cognitive psychology and task analysis exemplify such a philosophy. Complementary views are held by those who hold the sociological and the semiological as primary, and consider that objective categorical models are insufficient to model the complexity of human activity. Collaborative filtering, ecological psychology and ethnography are examples here. The techniques and systems presented in this book do not all lie towards one end of this spectrum, but instead show a variety of choices and emphases. This chapter, however, focuses on theory firmly towards the subjective and linguistic end of the spectrum: tools to let us place, compare and design techniques and systems. Such theory is noticeable by its near-absence in the literature of this burgeoning research area. Here we try to redress the balance, aiming to build a more abstract and general view of our work|||\N
3115|||Foundations of statistical natural language processing||| |||2000
32|||Using temporal profiles of queries for precision prediction|||A key missing component in information retrieval systems is self-diagnostic tests to establish whether the system can provide reasonable results for a given query on a document collection. If we can measure properties of a retrieved set of documents which allow us to predict average precision, we can automate the decision of whether to elicit relevance feedback, or modify the retrieval system in other ways. We use meta-data attached to documents in the form of time stamps to measure the distribution of documents retrieved in response to a query, over the time domain, to create a temporal profile for a query. We define some useful features over this temporal profile. We find that using these temporal features, together with the content of the documents retrieved, we can improve the prediction of average precision for a query.|||2004
321|||Time-based language models|||We explore the relationship between time and relevance using TREC ad-hoc queries. A type of query is identified that favors very recent documents. We propose a time-based language model approach to retrieval for these queries. We show how time can be incorporated into both query-likelihood models and relevance models. These models were used for experiments comparing time-based language models to heuristic techniques for incorporating document recency in the ranking. Our results show that time-based models perform as well as or better than the best of the heuristic techniques.|||2003
322|||Predicting query performance|||We develop a method for predicting query performance by computing the relative entropy between a query language model and the corresponding collection language model. The resulting clarity score measures the coherence of the language usage in documents whose models are likely to generate the query. We suggest that clarity scores measure the ambiguity of a query with respect to a collection of documents and show that they correlate positively with average precision in a variety of TREC test sets. Thus, the clarity score may be used to identify ineffective queries, on average, without relevance information. We develop an algorithm for automatically setting the clarity score threshold between predicted poorly-performing queries and acceptable queries and validate it using TREC data. In particular, we compare the automatic thresholds to optimum thresholds and also check how frequently results as good are achieved in sampling experiments that randomly assign queries to the two classes|||2002
323|||Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations|||Witten and Frank's textbook was one of two books that I used for a data mining class in the Fall of 2001.The book covers all major methods of data mining that produce a knowledge representation as output. Knowledge representation is hereby understood as a representation that can be studied, understood, and interpreted by human beings, at least in principle. Thus, neural networks and genetic algorithms are excluded from the topics of this textbook. We need to say "can be understood in principle" because a large decision tree or a large rule set may be as hard to interpret as a neural network. The book first develops the basic machine learning and data mining methods. These include decision trees, classification and association rules, support vector machines, instance-based learning, Naive Bayes classifiers, clustering, and numeric prediction based on linear regression, regression trees, and model trees. It then goes deeper into evaluation and implementation issues. Next it moves on to deeper coverage of issues such as attribute selection, discretization, data cleansing, and combinations of multiple models (bagging, boosting, and stacking). The final chapter deals with advanced topics such as visual machine learning, text mining, and Web mining.|||1999
324|||Bursty and hierarchical structure in streams|||A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a "burst of activity," with certain features rising sharply in frequency as the topic emerges.The goal of the present work is to develop a formal approach for modeling such "bursts," in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them.|||2002
325|||Viewing morphology as an inference process|||Morphology is the area of linguistics concerned with the internal structure of words. Information Retrieval has generally not paid much attention to word structure, other than to account for some of the variability in word forms via the use of stemmers. This paper will describe our experiments to determine the importance of morphology, and the effect that it has on performance. We will also describe the role of morphological analysis in word sense disambiguation, and in identifying lexical semantic relationships in a machine-readable dictionary. We will first provide a brief overview of morphological phenomena, and then describe the experiments themselves.|||1993
326|||TimeMines: Constructing Timelines with Statistical Models of Word Usage|||We present a system, TimeMines, that automatically generatestimelines from date-tagged free text corpora. TimeMinesdetects, ranks, and groups semantic features based ontheir statistical properties. We use these features to discoversets of related stories that deal with a single topic|||2000
327|||Relevance-Based Language Models: Estimation and Analysis|||It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model with no training data. We propose a novel technique for estimating such models us- ing the query alone. We demonstrate that our technique can produce highly accurate relevance models. Our ex- periments show relevance models outperforming baseline language modeling systems on TREC retrieval. The main contribution of this work is an effective formal method for estimating a relevance model with no training data|||\N
328|||The lemur toolkit for language modeling and information retrieval||| |||2003
33|||Improving the estimation of relevance models using large external corpora|||Information retrieval algorithms leverage various collection statistics to improve performance. Because these statistics are often computed on a relatively small evaluation corpus, we believe using larger, non-evaluation corpora should im- prove performance. Specifically, we advocate incorporating external corpora based on language modeling. We refer to this process as external expansion. When compared to tra- ditional pseudo-relevance feedback techniques, external ex- pansion is more stable across topics and up to 10% more eective in terms of mean average precision. Our results show that using a high quality corpus that is comparable to the evaluation corpus can be as, if not more, eective than using the web. Our results also show that external expansion outperforms simulated relevance feedback. In addition, we propose a method for predicting the extent to which external expansion will improve retrieval performance. Our new mea- sure demonstrates positive correlation with improvements in mean average precision.|||2006
331|||INQUERY Does Battle With TREC6|||Abstract: this report covers our approach to each of the tracks as well as some experimentalresults and analysis. We start with an overview of the major tools that were used across all tracks. The paperis divided into the following sections. The track descriptions are generally broken into approach, results, andanalysis sections, though some tracks require a different description|||1997
332|||The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter|||We observe a training set Q composed of l labeled samples {(X1,?1),...,(Xl, ?l )} and u unlabeled samples {X1',...,Xu'}. The labels ?i are independent random variables satisfying Pr{?i=1}=?, Pr{?i=2}=1-?. The labeled observations Xi are independently distributed with conditional density f?i(·) given ?i. Let (X0 ,?0) be a new sample, independently distributed as the samples in the training set. We observe X0 and we wish to infer the classification ?0. In this paper we first assume that the distributions f1(·) and f2(·) are given and that the mixing parameter is unknown. We show that the relative value of labeled and unlabeled samples in reducing the risk of optimal classifiers is the ratio of the Fisher informations they carry about the parameter ?. We then assume that two densities g1(·) and g2(·) are given, but we do not know whether g1(·)=f1 (·) and g2(·)=f2(·) or if the opposite holds, nor do we know ?. Thus the learning problem consists of both estimating the optimum partition of the observation space and assigning the classifications to the decision regions. Here, we show that labeled samples are necessary to construct a classification rule and that they are exponentially more valuable than unlabeled samples|||1996
333|||The impact of corpus size on question answering performance|||Using our question answering system, questions from the TREC 2001 evaluation were executed over a series of Web data collections, with the sizes of the collections increasing from 25 gigabytes up to nearly a terabyte|||2002
334|||Exploiting redundancy in question answering|||Our goal is to automatically answer brief factual questions of the form ``When was the Battle of Hastings?'' or ``Who wrote The Wind in the Willows?''. Since the answer to nearly any such question can now be found somewhere on the Web, the problem reduces to finding potential answers in large volumes of data and validating their accuracy. We apply a method for arbitrary passage retrieval to the first half of the problem and demonstrate that answer redundancy can be used to address the second half. The success of our approach depends on the idea that the volume of available Web data is large enough to supply the answer to most factual questions multiple times and in multiple contexts. A query is generated from a question and this query is used to select short passages that may contain the answer from a large collection of Web data. These passages are analyzed to identify candidate answers. The frequency of these candidates within the passages is used to ``vote'' for the most likely answer. The approach is experimentally tested on questions taken from the TREC-9 question-answering test collection. As an additional demonstration, the approach is extended to answer multiple choice trivia questions of the form typically asked in trivia quizzes and television game shows.|||2001
335|||Web question answering: is more always better?|||This paper describes a question answering system that is designed to capitalize on the tremendous amount of data that is now available online. Most question answering systems use a wide variety of linguistic resources. We focus instead on the redundancy available in large corpora as an important resource. We use this redundancy to simplify the query rewrites that we need to use, and to support answer mining from returned snippets. Our system performs quite well given the simplicity of the techniques being utilized. Experimental results show that question answering accuracy can be greatly improved by analyzing more and more matching passages. Simple passage ranking and n-gram extraction techniques work well in our system making it efficient to use with many backend retrieval engines.|||2002
336|||Transductive Inference for Text Classification using Support VectorMachines|||This paper introduces transductive support vector machines (TSVMs) for text classification. While regular support vector machines (SVMs) try to induce a general decision function for a learning task, TSVMs take into account a particular test set and try to minimize misclassifications of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classification. These theoretical findings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a 20th on some tasks. This work also proposes an algorithm for training TSVMs efficiently, handling 10,000 examples and more.|||1999
337|||Boosting Web Retrieval through Query Operations|||We explore the use of phrase and proximity terms in the con- text of web retrieval, which is dierent from traditional ad-hoc retrieval both in document structure and in query characteristics. We show that for this type of task, the usage of both phrase and proximity terms is highly beneficial for early precision as well as for overall retrieval eec- tiveness. We also analyze why phrase and proximity terms are far more eective for web retrieval than for ad-hoc retrieval.|||2005
338|||Improving the effectiveness of information retrieval with local context analysis|||Techniques for automatic query expansion have been extensively studied in information research as a means of addressing the word mismatch between queries and documents. These techniques can be categorized as either global or local. While global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top-ranked documents retrieved for a query. While local techniques have shown to be more effective that global techniques in general, existing local techniques are not robust and can seriously hurt retrieved when few of the retrieval documents are relevant. We propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the top-ranked documents. Experiments on a number of collections, both English and non-English, show that local context analysis offers more effective and consistent retrieval results.|||2000
41|||Learning a Monolingual Language Model from a Multilingual Text Database|||Language models are of importance in speech recognition,document classification, and database selection algorithms.Traditionally language models are learned from corpora specificallyacquired for the purpose. Increasingly, however, thereis interest in constructing language models for specific languagesfrom heterogeneous sources such as the web. Querybasedsampling has been shown to be effective for gaugingthe content of monolingual heterogeneous databases. Wepropose evaluating an...|||2000
411|||Statistical Identification of Language|||A statistically based program has been written which learns to distinguishbetween languages. The amount of training text that such a program needsis surprisingly small, and the amount of text needed to make an identificationis also quite small. The program incorporates no linguistic presuppositionsother than the assumption that text can be encoded as a string of bytes. Sucha program can be used to determine which language small bits of text are in.It also shows a potential for what might...|||1994
412|||An empirical study of smoothing techniques for language modeling|||We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented|||1999
413|||Statistical methods for speech recognition||| |||1997
414|||A Winnow-Based Approach to Context-Sensitive Spelling Correction|||A large class of machine-learning problems in natural language require the characterization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts refer to only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Winnow have been shown to have exceptionally good theoretical properties. We present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting "to" for "too", "casual" for "causal", etc. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) The primary reason that WinSpell outperforms BaySpell is that WinSpell learns a better linear separator; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.|||1998
415|||The Automatic Creation of Literature Abstracts|||Excerpts of technical papers and magazine articles that serve the purposes of conventional abstracts have been created entirely by automatic means. In the exploratory research described, the complete text of an article in machine-readable form is scanned by an IBM 704 data-processing machine and analyzed in accordance with a standard program. Statistical information derived from word frequency and distribution is used by the machine to compute a relative measure of significance, first for individual words and then for sentences. Sentences scoring highest in significance are extracted and printed out to become the "auto-abstract."|||1958
416|||Using reinforcement learning to spider the Web efficiently|||Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. The paper argues that the creation of efficient Web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive Bayes text classifier. Experiments on two real-world spidering tasks show a three-fold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.|||1999
417|||An Empirical Study of Smoothing Techniques for Language|||We present an extensive empirical comparisonof several smoothing techniques inthe domain of language modeling, includingthose described by Jelinek and Mercer(1980), Katz (1987), and Church andGale (1991). We investigate for the rsttime how factors such as training datasize, corpus (e.g., Brown versus Wall StreetJournal), and n-gram order (bigram versustrigram) aect the relative performance ofthese methods, which we measure throughthe cross-entropy of test data. In addition,we...|||1996
418|||Latent Semantic Indexing (LSI) and TREC2||| |||1993
419|||Autonomous ocean sampling networks||| |||1993
4110|||Automatic Discovery of Language Models for Text Databases|||The proliferation of text databases within large organiza- tions and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database se- lection algorithm such as GlOSS can provide assistance by automatically selecting appropriate databases for an infor- mation need. Current practice is that each database pro- vides its language model upon request, but this cooperative approach has important limitations. This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the nor- mal process of running queries and retrieving documents. Although random sampling is not possible, it can be ap- proximated with carefully selected queries. This sampling approach avoids the limitations that characterize the coop- erative approach, and also enables additional capabilities. Experimental results demonstrate that accurate language models can be learned from a relatively small number of queries and documents.|||1999
4111|||A Winnow-Based Approach to Context-Sensitive Spelling Correction|||A large class of machine-learning problems in natural language require the character- ization of linguistic context. Two characteristic properties of such problems are that their feature space is of very high dimensionality, and their target concepts depend on only a small subset of the features in the space. Under such conditions, multiplicative weight-update algorithms such as Win- now have been shown to have exceptionally good theoretical properties. In the work reported here, we present an algorithm combining variants of Winnow and weighted-majority voting, and apply it to a problem in the aforementioned class: context-sensitive spelling correction. This is the task of fixing spelling errors that happen to result in valid words, such as substituting to for too, casual for causal, and so on. We evaluate our algorithm, WinSpell, by comparing it against BaySpell, a statistics-based method representing the state of the art for this task. We find: (1) When run with a full (unpruned) set of features, WinSpell achieves accuracies significantly higher than BaySpell was able to achieve in either the pruned or unpruned condition; (2) When compared with other systems in the literature, WinSpell exhibits the highest performance; (3) While several aspects of WinSpell's architecture contribute to its superiority over BaySpell, the primary factor is that it is able to learn a better linear separator than BaySpell learns; (4) When run on a test set drawn from a different corpus than the training set was drawn from, WinSpell is better able than BaySpell to adapt, using a strategy we will present that combines supervised learning on the training set with unsupervised learning on the (noisy) test set.|||1999
42|||Online Learning for Web Query Generation: Finding Documents Matching a Minority Concept on the Web|||This paper describes an approach for learning to generateweb-search queries for collecting documents matching a minority concept.As a case study we use the concept of text documents belongingto Slovenian, a minority natural language on the Web. Individualdocuments are automatically labeled as relevant or non-relevant usinga language filter and the feedback is used to learn what query-lengthsand inclusion/exclusion term-selection methods are helpful for findingpreviously unseen...|||2001
421|||Document Categorization and Query Generation on the World Wide Web Using WebACE|||We present WebACE, an agent for exploring and categorizing documents on the World Wide Web based on a user profile. The heart of the agent is an unsupervised categorization of a set of documents, combined with a process for generating new queries that is used to search for new related documents and for filtering the resulting documents to extract the ones most closely related to the starting set. The document categories are not given a priori. We present the overall architecture and describe...|||1999
422|||Learning a monolingual language model from a multilingual text database|||The Web is a source of valuable information, but the process of collecting, organizing, and effectively utilizing the resources it contains is difficult. We describe CorpusBuilder, an approach for automatically generating Web search queries for collecting documents matching a minority concept. The concept used for this paper is that of text documents belonging to a minority natural language on the Web. Individual documents are automatically labeled as relevant or nonrelevant using a language filter, and the feedback is used to learn what query lengths and inclusion/exclusion term-selection methods are helpful for finding previously unseen documents in the target language. Our system learns to select good query terms using a variety of term scoring methods. Using odds ratio scores calculated over the documents acquired was one of the most consistently accurate query-generation methods. To reduce the number of estimated parameters, we parameterize the query length using a Gamma distribution and present empirical results with learning methods that vary the time horizon used when learning from the results of past queries. We find that our system performs well whether we initialize it with a whole document or with a handful of words elicited from a user. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes well across several languages regardless of the initial conditions.|||\N
423|||Improving Category Specific Web Search by Learning Query Modifications|||Users looking for documents within specific cat- egories may have a difficult time locating valuable documents using general purpose search engines. We present an automated method for learning query modifications that can dramatically improve preci- sion for locating pages within specified categories using web search engines. We also present a clas- sification procedure that can recognize pages in a specific category with high precision, using textual content, text location, and HTML structure. Evalu- ation shows that the approach is highly effective for locating personal homepages and calls for papers. These algorithms are used to improve category spe- cific search in the Inquirus 2 search engine.|||2001
424|||Feature selection for unbalanced class distribution and Naive Bayes|||This paper describes an approach to feature subset selection that takes into account problem specifics and learning algorithm characteristics. It is developed for the Naive Bayesian classifier applied on text data, since it combines well with the addressed learning problems. We focus on domains with many features that also have a highly unbalanced class distribution and asymmetric misclassification costs given only implicitly in the problem. By asymmetric misclassification costs we mean that one of the class values is the target class value for which we want to get predictions and we prefer false positive over false negative. Our example problem is automatic document categorization using machine learning, where we want to identify documents relevant for the selected category. Usually, only about 1\%-10\% of examples belong to the selected category. Our experimental comparison of eleven feature scoring measures show that considering domain and algorithm characteristics significantly improves the results of classification.|||1999
425|||WebSail: From On-Line Learning to Web Search|||In this paper we investigate the applicability of on-line learning algorithms to the real-world problem of web search. Consider that web documents are indexed using Boolean features. We first present a practically efficient on-line learn- ing algorithm TW2 to search for web documents represented by a disjunction of at most relevant features. We then design and implement WebSail, a real-time adaptive web search learner, with TW2 as its learning component. Web- Sail learns from the user's relevance feedback in real-time and helps the user to search for the desired web documents. The architecture and performance of WebSail are also dis- cussed.|||2000
426|||On-line Algorithms in Machine Learning||| The areas of On-Line Algorithms and Machine Learning areboth concerned with problems of making decisions about the presentbased only on knowledge of the past. Although these areas differ in termsof their emphasis and the problems typically studied, there are a collectionof results in Computational Learning Theory that fit nicely into the&quot;on-line algorithms&quot; framework. This survey article discusses some of theresults, models, and open problems from Computational Learning Theorythat seem ...|||1996
427|||N-Gram-Based Text Categorization|||Text categorization is a fundamental task in doc-ument processing, allowing the automated handling of enormous streams of documents in electronic form. One difficulty in handling some classes of documents is the presence of different kinds of textual errors, such as spelling and grammatical errors in email, and character recognition errors in documents that come through OCR. Text categorization must work reliably on all input, and thus must tolerate some level of these kinds of problems. We describe here an N-gram-based approach to text categorization that is tolerant of textual errors. The system is small, fast and robust. This system worked very well for language classification, achieving in one test a 99.8\% correct classification rate on Usenet newsgroup articles written in different languages. The system also worked reasonably well for classifying articles from a number of different computer-oriented newsgroups according to subject, achieving as high as an 80\% correct classification rate. There are also several obvious directions for improving the system's classification performance in those cases where it did not do as well. The system is based on calculating and comparing profiles of N-gram frequencies. First, we use the system to compute profiles on training set data that represent the various categories, e.g., language samples or newsgroup content samples. Then the system computes a profile for a particular document that is to be classified. Finally, the system computes a distance measure between the document's profile and each of the category profiles. The system selects the category whose profile has the smallest distance to the document's profile. The profiles involved are quite small, typically 10K bytes for a category training set, and less than 4K bytes for an individual document. Using N-gram frequency profiles provides a simple and reliable way to categorize documents in a wide range of classification tasks.|||1994
43|||Query word deletion prediction|||Web search query logs contain traces of users' search modifications. One strategy users employ is deleting terms, presumably to obtain greater coverage. It is useful to model and automate term deletion when arbitrary searches are conjunctively matched against a small hand constructed collection, such as a hand-built hierarchy, or collection of high-quality pages matched with key phrases. Queries with no matches can have words deleted till a match is obtained. We provide algorithms which perform substantially better than the baseline in predicting which word should be deleted from a reformulated query, for increasing query coverage in the context of web search on small high-quality collections|||2003
431|||Analysis of a Very Large AltaVista Query Log|||In this paper we present an analysis of a 280 GB AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents approximately 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. Furthermore we present results of a correlation analysis of the log entries, studying the interaction of terms within queries....|||1998
432|||Optimizing search engines using clickthrough data|||This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples|||2002
433|||Query expansion using local and global document analysis|||Automatic query expansion has long been suggested as a technique for dealing with the fundamental issue of word mismatch in information retrieval. A number of approaches to ezpanrnion have been studied and, more recently, attention has focused on techniques that analyze the corpus to discover word relationship (global techniques) and those that analyze documents retrieved by the initial query ( local feedback). In this paper, we compare the effectiveness of these approaches and show that, although global analysis haa some advantages,local analysia is generally more effective. We also show that using global analysis techniques, such as word contezt and phrase structure, on the local aet of documents produces results that are both more effective and more predictable than simple local feedback|||1996
434|||Patterns of Search: Analyzing and Modeling Web Query Refinement|||We discuss the construction of probabilistic models centering on temporal pat- terns of query refinement. Our analyses are derived from a large corpus of Web search queries extracted from server logs recorded by a popular Internet search service. We frame the modeling task in terms of pursuing an understanding of probabilistic relationships among temporal patterns of activity, informational goals, and classes of query refinement. We con- struct Bayesian networks that predict search behavior, with a focus on the progression of queries over time. We review a methodology for abstracting and tagging user queries. Af- ter presenting key statistics on query length, query frequency, and informational goals, we describe user models that capture the dynamics of query refinement. The evolution of the World Wide Web has provided rich opportunities for gathering and analyz- ing anonymous log data generated by user interactions with network-based services. Web-based search engines such as Excite, AltaVista, and Lycos provide search services by crawling and indexing large portions of the Web. In a typical session, a user enters a string of words into the search engine's input field and receives an HTML page containing a list of web documents matching the user's query. This list may include hundreds of documents. Web search engines rank this list and present the results in small groups. The user may follow one or more of the returned links, request additional results, or refine and resubmit a query. We review our work in developing models of users' search behaviors from log data. Our motivation is to enhance information retrieval by developing models with the ability to diagnose a user's informational goals. In this paper, we describe probabilistic models that are used to infer a probability distribution over user's goals from the time-stamped data available in server logs. More specifically, we elucidate probabilistic relationships between user goals and temporal patterns of query refinement activity. In contrast with the work of Maglio and Barrett (1997), who study several users' complete web searches, we consider only interactions with the search service itself. In distinction to the work of Silverstein et al. (1998), which reported statistics over a large corpus of unprocessed log data, we mine a corpus that has been hand-tagged to provide information about the refinements and goals associated with queries. We shall first describe the initial data set and our methodology for transforming the data into a representation of user behavior with richer semantics. We review a set of definitions that abstract queries into classes of query refinement and informational goals. We then present key statistics of our corpus. Finally, we describe the construction of Bayesian network models that capture dependencies among variables of interest.|||1998
435|||From E-Sex to E-Commerce: Web Search Changes|||The Web has become a worldwide source of information and a mainstream business tool. Are human information needs and searching behaviors evolving along with Web content? As part of a body of research studying this question, we have analyzed three data sets culled from more than one million queries submitted by more than 200,000 users of the Excite Web search engine, collected in September 1997, December 1999, and May 2001. This longitudinal benchmark study shows that public Web searching is evolving in certain directions. Specifically, search topics have shifted from entertainment and sex to commerce and people, but there is little change in query lengths or frequency per user. Search topics have shifted, but there is little change in user search behaviors|||2002
44|||Biasing web search results for topic familiarity|||Depending on a web searcher's familiarity with a query's target topic, it may be more appropriate to show her introductory or advanced documents. The TREC HARD [1] track defined topic familiarity as meta-data associated with a user's query. We instead define a user-independent and query-independent model of topic-familiarity required to read a document, so it can be matched to a given user in response to a query. An introductory web page is defined as A web page that doesn't presuppose any background knowledge of the topic it is on, and to an extent introduces or defines the key terms in the topic. while an advanced web page is defined as A web page that assumes sufficient background knowledge of the topic it is on, and familiarity with the key technical/ important terms in the topic, and potentially builds on them. We develop a method for biasing the initial mix of documents returned by a search engine to increase the number of documents of desired familiarity level up to position 5, and up to position 10. Our method involves building a supervised text classifier, incorporating features based on reading level, the distribution of stop-words in the text, and non-text features such as average line-length. Using this familiarity classifier, we achieve statistically significant improvements at reranking the result set to show introductory documents higher up the ranked list. Our classifier can be seamlessly integrated into current search engine technology without involving any major modifications to existing architectures.|||2005
441|||Smog grading - a new readability formula||| |||1969
442|||Derivation of new readability formulas for navy enlisted personnel||| |||1975
443|||A coefficient of agreement for nominal scales||| |||1960
444|||HARD Track Overview in TREC 2003: High Accuracy Retrieval from Documents|||Abstract The HARD track of TREC 2004 aims to improve the accuracy of information retrieval through the use of three techniques: (1) query metadata that better describes the information need, (2) focused and time-limited interaction with the searcher through \clarication forms", and (3) incorporation of passage-level relevance judgments,and retrieval. Participation in all three aspects of the track was excellent this year with about 10 groups trying something in each area. No group was able to achieve huge gains in eectiveness using these techniques, but some improvements were found and enthusiasm for the clarication,forms (in particular) remains high. The track will run again in TREC 2005.|||2003
45|||N Semantic Classes are Harder than Two|||We show that we can automatically clas- sify semantically related phrases into 10 classes. Classification robustness is im- proved by training with multiple sources of evidence, including within-document cooccurrence, HTML markup, syntactic relationships in sentences, substitutability in query logs, and string similarity. Our work provides a benchmark for automatic n-way classification into WordNet's se- mantic classes, both on a TREC news cor- pus and on a corpus of substitutable search query phrases.|||2006
451|||Using terminological feedback for web search refinement: a log-based study|||Although interactive query reformulation has been actively studied in the laboratory, little is known about the actual behavior of web searchers who are offered terminological feedback along with their search results. We analyze log sessions for two groups of users interacting with variants of the AltaVista search engine - a baseline group given no terminological feedback and a feedback group to whom twelve refinement terms are offered along with the search results. We examine uptake, refinement effectiveness, conditions of use, and refinement type preferences. Although our measure of overall session "success" shows no difference between outcomes for the two groups, we find evidence that a subset of those users presented with terminological feedback do make effective use of it on a continuing basis.|||2003
452|||The PASCAL Recognising Textual Entailment Challenge|||This paper describes the PASCAL Net- work of Excellence Recognising Textual Entailment (RTE) Challenge benchmark 1. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (en- tailed) from the other. This application- independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, sug- gesting the generic relevance of the task.|||2005
453|||Measures of Distributional Similarity|||We study distributional similarity measures for the purpose of improving probability estima- tion for unseen cooccurrences. Our contribu- tions are three-fold: an empirical comparison of a broad range of measures; a classification of similarity functions based on the information that they incorporate; and the introduction of a novel function that is superior at evaluating potential proxy distributions.|||1999
454|||A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs|||This paper develops a fast method for solving linear SVMs with L2 loss function that is suited for large scale data mining tasks such as text classification. Th is is done by modifying the finite Newton method of Mangasarian in several ways. Experiments indicate that the method is much faster than decomposition methods such as SVMlight, SMO and BSVM (e.g., 4-100 fold), especially when the number of examples is large. The paper also suggests ways of extending the method to other loss functions such as the modified Huber's loss function and the L1 loss function, and also for solving ordinal regression.|||2005
455|||Generating query substitutions|||We introduce the notion of query substitution, that is, gen- erating a new query to replace a user's original search query. Our technique uses modications based on typical substitu- tions web searchers make to their queries. In this way the new query is strongly related to the original query, contain- ing terms closely related to all of the original terms. This contrasts with query expansion through pseudo-relevance feedback, which is costly and can lead to query drift. This also contrasts with query relaxation through boolean or TFIDF retrieval, which reduces the specicit y of the query. We de- ne a scale for evaluating query substitution, and show that our method performs well at generating new queries related to the original queries. We build a model for selecting be- tween candidates, by using a number of features relating the query-candidate pair, and by tting the model to human judgments of relevance of query suggestions. This further improves the quality of the candidates generated. Experi- ments show that our techniques signican tly increase cover- age and eectiv eness in the setting of sponsored search.|||2006
456|||Accurate Methods for the Statistics of Surprise and Coincidence|||Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text|||1993
457|||Patterns and Sequences of Multiple Query Reformulations in Web Searching: A Preliminary Stud|||While some studies have investigated query reformulation in traditional online systems, there has been little understanding of how users reformulate their queries multiple times within search sessions on the Web. This paper reports on patterns and sequences of query reformulation based on query logs from a Web search engine. The data set contained only search sessions in which multiple query modifications were made. The analysis of data resulted in three facets of reformulation: content, format, and resource. Each facet was further categorized by 10 sub-facets. The results show that while most query reformulation involves content changes, about 15% of reformulation is related to format modifications. Six patterns of query reformulation emerged as a result of sequence analysis: specified reformulation, parallel reformulation, generalized reformulation, dynamic reformulation, format reformulation, and alternative reformulation. Each pattern is discussed with definitions and examples. The results indicate that both planned and situated aspects affect query reformulation in Web searching. The implications for new Web search engine tools and features are also discussed.|||2001
458|||Learning Syntactic Patterns for Automatic Hypernym Discovery|||Semantic taxonomies such as WordNet provide a rich source of knowl- edge for natural language processing applications, but are expensive to build, maintain, and extend. Motivated by the problem of automatically constructing and extending such taxonomies, in this paper we present a new algorithm for automatically learning hypernym (is-a) relations from text. Our method generalizes earlier work that had relied on using small numbers of hand-crafted regular expression patterns to identify hyper- nym pairs. Using ìdependency pathî features extracted from parse trees, we introduce a general-purpose formalization and generalization of these patterns. Given a training set of text containing known hypernym pairs, our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs. On our evaluation task (de- termining whether two nouns in a news article participate in a hypernym relationship), our automatically extracted database of hypernyms attains both higher precision and higher recall than WordNet.|||2004
459||| Scoring missing terms in information retrieval tasks|||An usual approach to address mismatching vocabulary problem is to augment the original query using dictionaries and other lexical resources and/or by looking at pseudo-relevant documents. Either way, terms are added to form a new query that will be used to score all documents in a subsequent retrieval pass, and as consequence the original query's focus may drift because of the newly added terms. We propose a new method to address the mismatching vocabulary problem, expanding original query terms only when necessary and complementing the user query for missing terms while scoring documents. It allows related semantic aspects to be included in a conservative and selective way, thus reducing the possibility of query drift. Our results using replacements for the missing query terms in modified document and passages retrieval methods show significant improvement over the original ones.|||2004
4510|||Automatic Acquisition of Hyponyms from Large Text Corpora|||We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.|||1992
46|||Automatically generating related queries in Japanese|||Web searchers reformulate their queries, as they adapt to search engine behavior, learn more about a topic, or simply correct typing errors. Automatic query rewriting can help user web search, by augmenting a user's query, or replacing the query with one likely to retrieve better results. One example of query-rewriting is spell-correction. We may also be interested in changing words to synonyms or other related terms. For Japanese, the opportunities for improving results are greater than for languages with a single character set, since documents may be written in multiple character sets, and a user may express the same meaning using different character sets. We give a description of the characteristics of Japanese search query logs and manual query reformulations carried out by Japanese web searchers. We use characteristics of Japanese query reformulations to extend previ- ous work on automatic query rewriting in English, taking into account the Japanese writing system. We intro- duce several new features for building models resulting from this difference and discuss their impact on auto- matic query rewriting. We also examine enhancements in the form of rules which block conversion between some character sets, to address Japanese homophones. The precision/recall curves show significant improvement with the new feature set and blocking rules, and are often better than the English counterpart|||2006
461|||Unity: relevance feedback using user query logs |||The exponential growth of the Web and the increasing ability of web search engines to index data have led to a problem of plenty. The number of results returned per query is typically in the order of millions of documents for many common queries. Although there is the benefit of added coverage for every query, the problem of ranking these documents and giving the best results gets worse. The problem is even more difficult in case of temporal and ambiguous queries. We try to address this problem using feedback from user query logs. We leverage a technology called Units for generating query refinements which are shown as Also try queries on Yahoo! Search. We consider these refinements as sub-concepts which help define user intent and use them to improve search relevance. The results obtained via live testing on Yahoo! Search are encouraging.|||2006
462|||An Automatic Translation System Of Non-Segmented Kana Sentences Into Kanji-Kana Sentences|||This paper presents the algorithms to solvethe two main problems comprised in the automaticKana-KanJi translation system, in which theinput sentences in Kana are translated intoordinary Japanese sentences in Kanji and Kana :the segmentation of non-segmented sentences intoBunsetsu and the word identification from homo-nyms. Employing this algorithm, non-segmentedKana input sentences could be automaticallytranslated into Kanji and Kana output sentenceswith 96.2 per cent succes|||1980
463|||Synchronous Morphological Analysis of Grapheme and Phoneme for Japanese OCR|||We developed a novel language model for Japanese based on grapheme-phoneme tuples, which is one order of magnitude smaller than word-based models. We also developed an alignment algorithm of graphemes and phonemes for both ordinary text and OCR output. We show, by experiment, that the combination of the grapheme-phoneme tuple ngram model and the grapheme-phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representations are given.|||2000
464|||Re-examining the potential effectiveness of interactive query expansion|||Much attention has been paid to the relative effectiveness of interactive query expansion versus automatic query expansion. Although interactive query expansion has the potential to be an effective means of improving a search, in this paper we show that, on average, human searchers are less likely than systems to make good expansion decisions. To enable good expansion decisions, searchers must have adequate instructions on how to use interactive query expansion functionalities. We show that simple instructions on using interactive query expansion do not necessarily help searchers make good expansion decisions and discuss difficulties found in making query expansion decisions.|||2003
465|||Improving retrieval performance by relevance feedback|||Relevance feedback is an automatic process, introduced over 20 years ago, designed to produce improved query formulations following an initial retrieval operation. The principal relevance feedback methods described over the years are examined briefly, and evaluation data are included to demonstrate the effectiveness of the various methods. Prescriptions are given for conducting text retrieval operations iteratively using relevance feedback.|||1990